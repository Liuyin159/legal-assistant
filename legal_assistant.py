"""
æ³•å¾‹åŠ©æ‰‹é›†æˆè„šæœ¬
åŸºäº Qwen3-1.7B + LoRA å¾®è°ƒçš„æ³•å¾‹æ¨¡å‹ï¼Œæä¾›å¤šç§æ³•å¾‹åœºæ™¯çš„æ™ºèƒ½åŠ©æ‰‹åŠŸèƒ½
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig  # æ–°å¢Peftç›¸å…³å¯¼å…¥
import argparse
import json
import time
from datetime import datetime
import os

# æ³•å¾‹ä¸“ä¸šæç¤ºè¯æ¨¡æ¿ï¼ˆä¿æŒä¸å˜ï¼‰
LAW_PROMPTS = {
    "civil": "ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„æ°‘äº‹å¾‹å¸ˆï¼Œè¯·æ ¹æ®æ¡ˆä»¶æè¿°ï¼Œæä¾›ä¸“ä¸šçš„æ³•å¾‹åˆ†æå’Œç»´æƒå»ºè®®ã€‚",
    "criminal": "ä½ æ˜¯ä¸€ååˆ‘äº‹è¾©æŠ¤å¾‹å¸ˆï¼Œè¯·åˆ†ææ¡ˆä»¶æƒ…å†µï¼Œæä¾›ä¸“ä¸šçš„æ³•å¾‹æ„è§å’Œè¾©æŠ¤ç­–ç•¥ã€‚",
    "contract": "ä½ æ˜¯ä¸€ååˆåŒæ³•å¾‹ä¸“å®¶ï¼Œè¯·å®¡æŸ¥åˆåŒæ¡æ¬¾ï¼Œæä¾›ä¸“ä¸šçš„æ³•å¾‹é£é™©è¯„ä¼°å’Œä¿®æ”¹å»ºè®®ã€‚",
    "labor": "ä½ æ˜¯ä¸€ååŠ³åŠ¨æ³•å¾‹å¸ˆï¼Œè¯·æ ¹æ®åŠ³åŠ¨çº çº·æƒ…å†µï¼Œæä¾›ä¸“ä¸šçš„æƒç›Šä¿æŠ¤å»ºè®®å’Œæ³•å¾‹é€”å¾„ã€‚",
    "intellectual_property": "ä½ æ˜¯ä¸€åçŸ¥è¯†äº§æƒå¾‹å¸ˆï¼Œè¯·æä¾›ä¸“ä¸šçš„ç‰ˆæƒã€ä¸“åˆ©ã€å•†æ ‡ä¿æŠ¤å»ºè®®ã€‚",
    "corporate": "ä½ æ˜¯ä¸€åå…¬å¸æ³•å¾‹å¸ˆï¼Œè¯·å°±å…¬å¸è®¾ç«‹ã€è¿è¥ã€æ²»ç†ç­‰æä¾›ä¸“ä¸šæ³•å¾‹æ„è§ã€‚",
    "family": "ä½ æ˜¯ä¸€åå©šå§»å®¶åº­æ³•å¾‹å¸ˆï¼Œè¯·å°±å©šå§»ã€ç»§æ‰¿ã€æŠšå…»ç­‰é—®é¢˜æä¾›ä¸“ä¸šæ³•å¾‹å»ºè®®ã€‚",
    "property": "ä½ æ˜¯ä¸€åç‰©æƒæ³•å¾‹ä¸“å®¶ï¼Œè¯·å°±æˆ¿äº§ã€åœŸåœ°ç­‰è´¢äº§æƒåˆ©æä¾›ä¸“ä¸šæ³•å¾‹åˆ†æã€‚",
    "administrative": "ä½ æ˜¯ä¸€åè¡Œæ”¿æ³•å¾‹å¸ˆï¼Œè¯·å°±è¡Œæ”¿å¤„ç½šã€è¡Œæ”¿è®¸å¯ç­‰æä¾›ä¸“ä¸šæ³•å¾‹æ„è§ã€‚",
    "international": "ä½ æ˜¯ä¸€åå›½é™…æ³•å¾‹å¸ˆï¼Œè¯·å°±è·¨å›½æ³•å¾‹äº‹åŠ¡æä¾›ä¸“ä¸šå»ºè®®ã€‚"
}

# å¸¸è§æ³•å¾‹åœºæ™¯ï¼ˆä¿æŒä¸å˜ï¼‰
LAW_SCENARIOS = {
    "1": "æ°‘äº‹çº çº·",
    "2": "åˆ‘äº‹æ¡ˆä»¶", 
    "3": "åˆåŒå®¡æŸ¥",
    "4": "åŠ³åŠ¨çº çº·",
    "5": "çŸ¥è¯†äº§æƒ",
    "6": "å…¬å¸äº‹åŠ¡",
    "7": "å©šå§»å®¶åº­",
    "8": "è´¢äº§æƒç›Š",
    "9": "è¡Œæ”¿äº‰è®®",
    "10": "å›½é™…æ³•å¾‹"
}

# é¢„è®¾é—®é¢˜ç¤ºä¾‹ï¼ˆä¿æŒä¸å˜ï¼‰
SAMPLE_QUESTIONS = {
    "civil": [
        "é‚»å±…è£…ä¿®å¯¼è‡´æˆ‘å®¶å¢™å£å¼€è£‚ï¼Œåº”è¯¥å¦‚ä½•ç»´æƒï¼Ÿ",
        "å€Ÿæ¬¾ç»™æœ‹å‹æ²¡æœ‰å†™å€Ÿæ¡ï¼Œç°åœ¨å¯¹æ–¹ä¸è¿˜é’±æ€ä¹ˆåŠï¼Ÿ",
        "ç½‘è´­å•†å“å­˜åœ¨è´¨é‡é—®é¢˜ï¼Œå•†å®¶æ‹’ç»é€€è´§è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ"
    ],
    "criminal": [
        "è¢«æŒ‡æ§ç›—çªƒä½†è¯æ®ä¸è¶³ï¼Œåº”è¯¥å¦‚ä½•è¾©æŠ¤ï¼Ÿ",
        "æ­£å½“é˜²å«çš„è®¤å®šæ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "åˆ‘äº‹æ¡ˆä»¶ä¸­å¦‚ä½•ç”³è¯·å–ä¿å€™å®¡ï¼Ÿ"
    ],
    "contract": [
        "ç­¾è®¢æˆ¿å±‹ç§ŸèµåˆåŒéœ€è¦æ³¨æ„å“ªäº›æ¡æ¬¾ï¼Ÿ",
        "åŠ³åŠ¨åˆåŒä¸­çš„ç«ä¸šé™åˆ¶æ¡æ¬¾æ˜¯å¦æœ‰æ•ˆï¼Ÿ",
        "å¦‚ä½•å®¡æŸ¥æŠ•èµ„åè®®ä¸­çš„é£é™©æ¡æ¬¾ï¼Ÿ"
    ],
    "labor": [
        "å…¬å¸æ— æ•…è¾é€€å‘˜å·¥ï¼Œåº”è¯¥æ€ä¹ˆç»´æƒï¼Ÿ",
        "åŠ ç­è´¹åº”è¯¥å¦‚ä½•è®¡ç®—å’Œä¸»å¼ ï¼Ÿ",
        "å·¥ä¼¤è®¤å®šéœ€è¦æä¾›å“ªäº›ææ–™ï¼Ÿ"
    ],
    "intellectual_property": [
        "å¦‚ä½•ç”³è¯·è½¯ä»¶è‘—ä½œæƒä¿æŠ¤ï¼Ÿ",
        "å‘ç°ä»–äººä¾µæƒä½¿ç”¨æˆ‘çš„å•†æ ‡è¯¥æ€ä¹ˆåŠï¼Ÿ",
        "ä¸“åˆ©è¢«ä¾µæƒåº”è¯¥å¦‚ä½•æ”¶é›†è¯æ®ï¼Ÿ"
    ],
    "corporate": [
        "æœ‰é™è´£ä»»å…¬å¸å’Œè‚¡ä»½æœ‰é™å…¬å¸æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "å…¬å¸è‚¡ä¸œä¹‹é—´å‘ç”Ÿçº çº·å¦‚ä½•è§£å†³ï¼Ÿ",
        "å…¬å¸å¹¶è´­éœ€è¦æ³¨æ„å“ªäº›æ³•å¾‹é£é™©ï¼Ÿ"
    ],
    "family": [
        "ç¦»å©šæ—¶å¤«å¦»å…±åŒè´¢äº§å¦‚ä½•åˆ†å‰²ï¼Ÿ",
        "é—å˜±ç»§æ‰¿å’Œæ³•å®šç»§æ‰¿æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "å­å¥³æŠšå…»æƒçš„åˆ¤å®šæ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
    ],
    "property": [
        "è´­ä¹°äºŒæ‰‹æˆ¿éœ€è¦æ³¨æ„å“ªäº›æ³•å¾‹é—®é¢˜ï¼Ÿ",
        "æˆ¿å±‹æ‹†è¿è¡¥å¿æ ‡å‡†å¦‚ä½•ç¡®å®šï¼Ÿ",
        "å°åŒºå…¬å…±åŒºåŸŸçš„æƒç›Šå½’å±å¦‚ä½•ç•Œå®šï¼Ÿ"
    ],
    "administrative": [
        "å¯¹è¡Œæ”¿å¤„ç½šå†³å®šä¸æœå¦‚ä½•ç”³è¯·è¡Œæ”¿å¤è®®ï¼Ÿ",
        "è¡Œæ”¿è®¸å¯è¢«æ‹’ç»åº”è¯¥æ€ä¹ˆåŠï¼Ÿ",
        "æ”¿åºœä¿¡æ¯å…¬å¼€ç”³è¯·è¢«é©³å›å¦‚ä½•æ•‘æµï¼Ÿ"
    ],
    "international": [
        "è·¨å›½è´¸æ˜“åˆåŒé€‚ç”¨å“ªå›½æ³•å¾‹ï¼Ÿ",
        "åœ¨æµ·å¤–æŠ•èµ„éœ€è¦æ³¨æ„å“ªäº›æ³•å¾‹é£é™©ï¼Ÿ",
        "å›½é™…ä»²è£ä¸è¯‰è®¼æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]
}

class LawAssistant:
    def __init__(self, base_model_path=None, lora_path=None, merged_model_path=None):
        """
        åˆå§‹åŒ–æ³•å¾‹åŠ©æ‰‹
        
        Args:
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            lora_path: LoRAé€‚é…å™¨è·¯å¾„
            merged_model_path: åˆå¹¶åçš„æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœå·²åˆå¹¶ï¼‰
        """
        self.base_model_path = base_model_path
        self.lora_path = lora_path
        self.merged_model_path = merged_model_path
        self.device, self.dtype = self._select_device_and_dtype()
        self.model = None
        self.tokenizer = None
        self.conversation_history = []
        
    def _select_device_and_dtype(self):
        """é€‰æ‹©è®¾å¤‡å’Œæ•°æ®ç±»å‹"""
        if torch.cuda.is_available():
            try:
                major, _ = torch.cuda.get_device_capability()
                if major >= 12:
                    raise RuntimeError("Unsupported CUDA capability for current PyTorch")
                _ = torch.zeros(1, device="cuda")
                return "cuda", torch.float16
            except Exception:
                pass
        return "cpu", torch.float32
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ - æ”¯æŒLoRAå¾®è°ƒæ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½æ³•å¾‹åŠ©æ‰‹æ¨¡å‹...")
        
        # ä¼˜å…ˆçº§1: ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹ï¼ˆæœ€ä¼˜é€‰æ‹©ï¼‰
        if self.merged_model_path and os.path.exists(self.merged_model_path):
            print("ğŸ”§ åŠ è½½åˆå¹¶åçš„æ¨¡å‹...")
            self._load_merged_model()
        # ä¼˜å…ˆçº§2: ä½¿ç”¨åŸºç¡€æ¨¡å‹ + LoRAé€‚é…å™¨
        elif self.base_model_path and self.lora_path and os.path.exists(self.base_model_path) and os.path.exists(self.lora_path):
            print("ğŸ”§ åŠ è½½åŸºç¡€æ¨¡å‹ + LoRAé€‚é…å™¨...")
            self._load_lora_model()
        # ä¼˜å…ˆçº§3: å›é€€åˆ°åªä½¿ç”¨åŸºç¡€æ¨¡å‹
        elif self.base_model_path and os.path.exists(self.base_model_path):
            print("âš ï¸ ä½¿ç”¨å›é€€æ¨¡å¼ï¼šä»…åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆæ— LoRAï¼‰...")
            self._load_base_model_only()
        else:
            raise ValueError("""
è¯·æä¾›æœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„ï¼š
1. åˆå¹¶æ¨¡å‹è·¯å¾„ (--merged-model)
2. åŸºç¡€æ¨¡å‹è·¯å¾„ + LoRAè·¯å¾„ (--base-model å’Œ --lora-path)  
3. ä»…åŸºç¡€æ¨¡å‹è·¯å¾„ (--base-model)
            """)
        
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _load_merged_model(self):
        """åŠ è½½åˆå¹¶åçš„æ¨¡å‹"""
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.merged_model_path):
            raise FileNotFoundError(f"åˆå¹¶æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.merged_model_path}")
        
        print(f"ğŸ“¥ ä» {self.merged_model_path} åŠ è½½åˆå¹¶æ¨¡å‹...")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.merged_model_path, 
            use_fast=False, 
            trust_remote_code=True,
            local_files_only=True
        )
        if self.tokenizer.pad_token is None and self.tokenizer.eos_token is not None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.merged_model_path, 
            torch_dtype=self.dtype,
            local_files_only=True
        )
        self.model.to(self.device)
        self.model.eval()
        
        print("âœ… åˆå¹¶æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    def _load_lora_model(self):
        """åŠ è½½åŸºç¡€æ¨¡å‹ + LoRAé€‚é…å™¨"""
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.base_model_path):
            raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.base_model_path}")
        if not os.path.exists(self.lora_path):
            raise FileNotFoundError(f"LoRAé€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {self.lora_path}")
        
        print(f"ğŸ“¥ ä» {self.base_model_path} åŠ è½½åŸºç¡€æ¨¡å‹...")
        print(f"ğŸ“¥ ä» {self.lora_path} åŠ è½½LoRAé€‚é…å™¨...")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_path, 
            use_fast=False, 
            trust_remote_code=True,
            local_files_only=True
        )
        if self.tokenizer.pad_token is None and self.tokenizer.eos_token is not None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path, 
            torch_dtype=self.dtype,
            local_files_only=True
        )
        
        # åŠ è½½LoRAé€‚é…å™¨
        self.model = PeftModel.from_pretrained(
            base_model,
            self.lora_path,
            torch_dtype=self.dtype,
            local_files_only=True
        )
        
        self.model.to(self.device)
        self.model.eval()
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"âœ… LoRAæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: å¯è®­ç»ƒ {trainable_params:,} / æ€»è®¡ {total_params:,}")
    
    def _load_base_model_only(self):
        """å›é€€æ¨¡å¼ï¼šä»…åŠ è½½åŸºç¡€æ¨¡å‹"""
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.base_model_path):
            raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.base_model_path}")
        
        print(f"ğŸ“¥ ä» {self.base_model_path} åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆæ— LoRAï¼‰...")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_path, 
            use_fast=False, 
            trust_remote_code=True,
            local_files_only=True
        )
        if self.tokenizer.pad_token is None and self.tokenizer.eos_token is not None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path, 
            torch_dtype=self.dtype,
            local_files_only=True
        )
        self.model.to(self.device)
        self.model.eval()
        
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆæœªä½¿ç”¨LoRAå¾®è°ƒï¼‰")
        print("ğŸ’¡ æç¤ºï¼šè¦ä½¿ç”¨LoRAå¾®è°ƒæ•ˆæœï¼Œè¯·æä¾› --lora-path å‚æ•°")
    
    def predict(self, messages, max_new_tokens=512):
        """æ‰§è¡Œé¢„æµ‹"""
        model_device = next(self.model.parameters()).device
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        inputs = self.tokenizer([text], return_tensors="pt")
        input_ids = inputs.input_ids.to(model_device)
        attention_mask = inputs.attention_mask.to(model_device) if hasattr(inputs, "attention_mask") else None

        generated = self.model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            do_sample=True,  # æ·»åŠ é‡‡æ ·ä»¥è·å¾—æ›´è‡ªç„¶çš„è¾“å‡º
            temperature=0.7,  # æ§åˆ¶åˆ›é€ æ€§
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id
        )

        # åªè§£ç æ–°ç”Ÿæˆéƒ¨åˆ†
        new_tokens = generated[:, input_ids.shape[1]:]
        response = self.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0]
        return response
    
    # ä»¥ä¸‹æ–¹æ³•ä¿æŒä¸å˜...
    def ask_question(self, question, scenario_type="civil", max_tokens=512):
        """è¯¢é—®æ³•å¾‹é—®é¢˜"""
        if scenario_type not in LAW_PROMPTS:
            scenario_type = "civil"
        
        messages = [
            {"role": "system", "content": LAW_PROMPTS[scenario_type]},
            {"role": "user", "content": question}
        ]
        
        # è®°å½•å¯¹è¯å†å²
        self.conversation_history.append({
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "scenario": scenario_type,
            "question": question,
            "response": None
        })
        
        response = self.predict(messages, max_new_tokens=max_tokens)
        
        # æ›´æ–°å¯¹è¯å†å²
        self.conversation_history[-1]["response"] = response
        
        return response
    
    def show_scenarios(self):
        """æ˜¾ç¤ºå¯ç”¨çš„æ³•å¾‹åœºæ™¯"""
        print("\nâš–ï¸ æ³•å¾‹åŠ©æ‰‹ - å¯ç”¨åœºæ™¯:")
        print("=" * 50)
        for key, value in LAW_SCENARIOS.items():
            print(f"{key:2}. {value}")
        print("=" * 50)
    
    def show_sample_questions(self, scenario_type):
        """æ˜¾ç¤ºç¤ºä¾‹é—®é¢˜"""
        if scenario_type in SAMPLE_QUESTIONS:
            print(f"\nğŸ“‹ {LAW_SCENARIOS.get(scenario_type, 'æ³•å¾‹å’¨è¯¢')} - ç¤ºä¾‹é—®é¢˜:")
            print("-" * 40)
            for i, question in enumerate(SAMPLE_QUESTIONS[scenario_type], 1):
                print(f"{i}. {question}")
            print("-" * 40)
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("\nğŸ¤– æ³•å¾‹åŠ©æ‰‹å·²å¯åŠ¨ï¼")
        print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        
        while True:
            try:
                # æ˜¾ç¤ºåœºæ™¯é€‰æ‹©
                self.show_scenarios()
                
                # é€‰æ‹©åœºæ™¯
                scenario_choice = input("\nè¯·é€‰æ‹©æ³•å¾‹åœºæ™¯ (1-10): ").strip()
                if scenario_choice == 'quit':
                    break
                elif scenario_choice == 'help':
                    self.show_help()
                    continue
                elif scenario_choice not in LAW_SCENARIOS:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                    continue
                
                # è·å–åœºæ™¯ç±»å‹
                scenario_type = list(LAW_PROMPTS.keys())[int(scenario_choice) - 1]
                
                # æ˜¾ç¤ºç¤ºä¾‹é—®é¢˜
                self.show_sample_questions(scenario_type)
                
                # è·å–ç”¨æˆ·é—®é¢˜
                question = input(f"\nè¯·è¾“å…¥æ‚¨çš„{LAW_SCENARIOS[scenario_choice]}é—®é¢˜: ").strip()
                if not question:
                    print("âŒ é—®é¢˜ä¸èƒ½ä¸ºç©º")
                    continue
                
                # ç”Ÿæˆå›ç­”
                print("\nğŸ”„ æ­£åœ¨åˆ†ææ‚¨çš„æ³•å¾‹é—®é¢˜...")
                start_time = time.time()
                
                response = self.ask_question(question, scenario_type)
                
                end_time = time.time()
                
                # æ˜¾ç¤ºå›ç­”
                elapsed_time = end_time - start_time
                print(f"\nğŸ’¡ æ³•å¾‹åŠ©æ‰‹å›ç­” (è€—æ—¶: {elapsed_time:.2f}ç§’):")
                print("=" * 60)
                print(response)
                print("=" * 60)
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                continue_choice = input("\næ˜¯å¦ç»§ç»­å’¨è¯¢ï¼Ÿ(y/n): ").strip().lower()
                if continue_choice in ['n', 'no', 'å¦']:
                    break
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨æ³•å¾‹åŠ©æ‰‹ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
                continue
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\nğŸ“– æ³•å¾‹åŠ©æ‰‹ä½¿ç”¨å¸®åŠ©:")
        print("=" * 50)
        print("1. é€‰æ‹©æ³•å¾‹åœºæ™¯ (1-10)")
        print("2. è¾“å…¥æ‚¨çš„æ³•å¾‹é—®é¢˜")
        print("3. è·å¾—ä¸“ä¸šçš„æ³•å¾‹å»ºè®®")
        print("\nğŸ’¡ é‡è¦æç¤º:")
        print("- æœ¬åŠ©æ‰‹ä»…æä¾›æ³•å¾‹çŸ¥è¯†å‚è€ƒï¼Œä¸æ„æˆæ­£å¼æ³•å¾‹æ„è§")
        print("- å…·ä½“æ¡ˆä»¶è¯·å’¨è¯¢æ‰§ä¸šå¾‹å¸ˆ")
        print("- è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
        print("=" * 50)
    
    def save_conversation(self, filename=None):
        """ä¿å­˜å¯¹è¯å†å²"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"law_conversation_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å¯¹è¯å†å²å·²ä¿å­˜åˆ°: {filename}")
    
    def batch_questions(self, questions_file):
        """æ‰¹é‡å¤„ç†é—®é¢˜"""
        try:
            with open(questions_file, 'r', encoding='utf-8') as f:
                questions = json.load(f)
            
            print(f"ğŸ“ å¼€å§‹æ‰¹é‡å¤„ç† {len(questions)} ä¸ªæ³•å¾‹é—®é¢˜...")
            
            results = []
            for i, q in enumerate(questions, 1):
                print(f"\nå¤„ç†ç¬¬ {i}/{len(questions)} ä¸ªé—®é¢˜...")
                response = self.ask_question(
                    q.get('question', ''), 
                    q.get('scenario', 'civil'),
                    q.get('max_tokens', 512)
                )
                
                results.append({
                    "question": q.get('question', ''),
                    "scenario": q.get('scenario', 'civil'),
                    "response": response
                })
            
            # ä¿å­˜ç»“æœ
            output_file = f"law_batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")


def main():
    parser = argparse.ArgumentParser(description="æ³•å¾‹åŠ©æ‰‹ - åŸºäºQwen3 + LoRAå¾®è°ƒçš„æ™ºèƒ½æ³•å¾‹å’¨è¯¢ç³»ç»Ÿ")
    
    # æ¨¡å‹è·¯å¾„å‚æ•°ç»„
    model_group = parser.add_argument_group('æ¨¡å‹è·¯å¾„è®¾ç½®')
    model_group.add_argument("--base-model", type=str, 
                           default="Qwen/Qwen3-1.7B",
                           help="åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–åç§°")
    model_group.add_argument("--lora-path", type=str, 
                           help="LoRAé€‚é…å™¨è·¯å¾„")
    model_group.add_argument("--merged-model", type=str, 
                           help="åˆå¹¶åçš„æ¨¡å‹è·¯å¾„")
    
    # åŠŸèƒ½å‚æ•°
    parser.add_argument("--question", "-q", type=str, 
                       help="ç›´æ¥è¯¢é—®é—®é¢˜ï¼ˆéœ€è¦é…åˆ --scenario ä½¿ç”¨ï¼‰")
    parser.add_argument("--scenario", "-s", type=str, 
                       default="civil", 
                       choices=list(LAW_PROMPTS.keys()),
                       help="æ³•å¾‹åœºæ™¯ç±»å‹")
    parser.add_argument("--max-tokens", "-m", type=int, 
                       default=512, 
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--batch", "-b", type=str, 
                       help="æ‰¹é‡å¤„ç†é—®é¢˜æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰")
    parser.add_argument("--save-history", action="store_true", 
                       help="ä¿å­˜å¯¹è¯å†å²")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ³•å¾‹åŠ©æ‰‹å®ä¾‹
    assistant = LawAssistant(
        base_model_path=args.base_model,
        lora_path=args.lora_path,
        merged_model_path=args.merged_model
    )
    
    # åŠ è½½æ¨¡å‹
    assistant.load_model()
    
    if args.batch:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        assistant.batch_questions(args.batch)
    elif args.question:
        # å•æ¬¡é—®ç­”æ¨¡å¼
        print(f"ğŸ¤– æ³•å¾‹åŠ©æ‰‹å›ç­”:")
        print("=" * 50)
        response = assistant.ask_question(args.question, args.scenario, args.max_tokens)
        print(response)
        print("=" * 50)
    else:
        # äº¤äº’æ¨¡å¼
        assistant.interactive_mode()
    
    # ä¿å­˜å¯¹è¯å†å²
    if args.save_history and assistant.conversation_history:
        assistant.save_conversation()


if __name__ == "__main__":
    main()